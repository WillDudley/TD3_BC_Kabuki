{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0852e6e1",
   "metadata": {},
   "source": [
    "## Define a naive replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff334c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "\t\t)\n",
    "\n",
    "\n",
    "\tdef convert_D4RL(self, dataset, dataset_size):\n",
    "\t\tself.state = dataset['observations'][:dataset_size]\n",
    "\t\tself.action = dataset['actions'][:dataset_size]\n",
    "\t\tself.next_state = dataset['next_observations'][:dataset_size]\n",
    "\t\tself.reward = dataset['rewards'][:dataset_size].reshape(-1,1)\n",
    "\t\tself.not_done = 1. - dataset['terminals'][:dataset_size].reshape(-1,1)\n",
    "\t\tself.size = self.state.shape[0]\n",
    "\n",
    "\n",
    "\tdef normalize_states(self, eps = 1e-3):\n",
    "\t\tmean = self.state.mean(0,keepdims=True)\n",
    "\t\tstd = self.state.std(0,keepdims=True) + eps\n",
    "\t\tself.state = (self.state - mean)/std\n",
    "\t\tself.next_state = (self.next_state - mean)/std\n",
    "\t\treturn mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a941ec",
   "metadata": {},
   "source": [
    "## Define TD3 architecture with BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859779e5-456d-4dfd-ab1f-fa43aad9bec8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action, l_repr_dim = 16):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "\n",
    "        self.l_repr = nn.Linear(256, l_repr_dim)\n",
    "        self.l3 = nn.Linear(l_repr_dim, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def encode(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return F.relu(self.l_repr(a))\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        a = F.relu(self.l_repr(a))\n",
    "        return self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, l_repr_dim = 16):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "\n",
    "        self.l_repr = nn.Linear(256, l_repr_dim)\n",
    "        self.l3 = nn.Linear(l_repr_dim, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "\n",
    "\n",
    "        self.l_repr2 = nn.Linear(256, l_repr_dim)\n",
    "        self.l6 = nn.Linear(l_repr_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = F.relu(self.l_repr(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = F.relu(self.l_repr2(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "    def encode(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        \n",
    "        return F.relu(self.l_repr(q1))\n",
    "    \n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = F.relu(self.l_repr(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1\n",
    "\n",
    "\n",
    "class TD3_BC(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_dim,\n",
    "            action_dim,\n",
    "            max_action,\n",
    "            discount=0.99,\n",
    "            tau=0.005,\n",
    "            policy_noise=0.2,\n",
    "            noise_clip=0.5,\n",
    "            policy_freq=2,\n",
    "            alpha=2.5,\n",
    "    ):\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "    def select_action(self, state, device=None):\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=256):\n",
    "        self.total_it += 1\n",
    "\n",
    "        # Sample replay buffer\n",
    "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (\n",
    "                    torch.randn_like(action) * self.policy_noise\n",
    "            ).clamp(-self.noise_clip, self.noise_clip)\n",
    "\n",
    "            next_action = (\n",
    "                    self.actor_target(next_state) + noise\n",
    "            ).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\n",
    "            # Compute actor loss\n",
    "            pi = self.actor(state)\n",
    "            Q = self.critic.Q1(state, pi)\n",
    "            lmbda = self.alpha / Q.abs().mean().detach()\n",
    "\n",
    "            actor_loss = -lmbda * Q.mean() + F.mse_loss(pi, action)\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        self.actor_target = copy.deepcopy(self.actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0351ec",
   "metadata": {},
   "source": [
    "## Define evaluation function and script parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63bc9026-8640-4d0e-b3a7-73bb5bc2afbc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "Warning: Mujoco-based envs failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'mjrl'\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "/home/will/PycharmProjects/TD3_BC_Kabuki/venv/lib/python3.9/site-packages/gym/envs/registration.py:415: UserWarning: \u001B[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001B[0m\n",
      "  logger.warn(\n",
      "pybullet build time: May 20 2022 19:45:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all import loaded\n"
     ]
    }
   ],
   "source": [
    "# Add mujoco stuff to path\n",
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/home/will/.mujoco/mujoco210/bin:/usr/lib/nvidia\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import argparse\n",
    "import os\n",
    "import d4rl\n",
    "# import kabuki \n",
    "# import gymnasium\n",
    "\n",
    "\n",
    "import utils\n",
    "import TD3_BC\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"all import loaded\")\n",
    "\n",
    "\n",
    "# Runs policy for X episodes and returns average reward\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, env_name, seed, mean, std, seed_offset=100, eval_episodes=10):\n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env.seed(seed + seed_offset)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(seed=args.seed), False\n",
    "        while not done:\n",
    "            state = (np.array(state).reshape(1,-1) - mean)/std\n",
    "            action = policy.select_action(state)\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "    d4rl_score = eval_env.get_normalized_score(avg_reward) * 100\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {d4rl_score:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return d4rl_score\n",
    "\n",
    "\n",
    "def get_parser():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Experiment\n",
    "    parser.add_argument(\"--policy\", default=\"TD3_BC\")               # Policy name\n",
    "    parser.add_argument(\"--env\", default=\"hopper-medium-v0\")        # OpenAI gym environment name\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)              # Sets Gym, PyTorch and Numpy seeds\n",
    "    parser.add_argument(\"--dataset_size\", default=1e6, type=int)       # How often (time steps) we evaluate\n",
    "    parser.add_argument(\"--eval_freq\", default=5e3, type=int)       # How often (time steps) we evaluate\n",
    "    parser.add_argument(\"--save_freq\", default=5e4, type=int)       # How often (time steps) we evaluate\n",
    "    parser.add_argument(\"--max_timesteps\", default=1e6, type=int)   # Max time steps to run environment\n",
    "    parser.add_argument(\"--save_model\", action=\"store_true\")        # Save model and optimizer parameters\n",
    "    parser.add_argument(\"--save_checkpoint\", action=\"store_true\")        # Save model and optimizer parameters\n",
    "    parser.add_argument(\"--load_model\", default=\"\")                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "    # TD3\n",
    "    parser.add_argument(\"--expl_noise\", default=0.1)                # Std of Gaussian exploration noise\n",
    "    parser.add_argument(\"--batch_size\", default=256, type=int)      # Batch size for both actor and critic\n",
    "    parser.add_argument(\"--discount\", default=0.99)                 # Discount factor\n",
    "    parser.add_argument(\"--tau\", default=0.005)                     # Target network update rate\n",
    "    parser.add_argument(\"--policy_noise\", default=0.2)              # Noise added to target policy during critic update\n",
    "    parser.add_argument(\"--noise_clip\", default=0.5)                # Range to clip target policy noise\n",
    "    parser.add_argument(\"--policy_freq\", default=2, type=int)       # Frequency of delayed policy updates\n",
    "    # TD3 + BC\n",
    "    parser.add_argument(\"--alpha\", default=2.5)\n",
    "    parser.add_argument(\"--normalize\", default=True)\n",
    "\n",
    "    return parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce55153",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629d9946-2ed8-46b1-9ab3-bfeddf0d924e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Policy: TD3_BC_100k, Env: HalfCheetah-v4, Seed: 0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = get_parser()\n",
    "args = parser.parse_args(\"--policy TD3_BC_100k --env HalfCheetah-v4 --seed 0 --dataset_size 100000 --eval_freq 5000 --save_freq 5000 --max_timesteps 100000 --save_model --save_checkpoint\".split(\" \"))\n",
    "\n",
    "file_name = f\"{args.policy}_{args.env}_{args.seed}\"\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Policy: {args.policy}, Env: {args.env}, Seed: {args.seed}\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "STORAGE_DIR = os.path.expanduser(\"~/.offlineRL/td3\")\n",
    "if not os.path.exists(f\"{STORAGE_DIR}/results\"):\n",
    "    os.makedirs(f\"{STORAGE_DIR}/results\")\n",
    "\n",
    "if args.save_model and not os.path.exists(f\"{STORAGE_DIR}/models\"):\n",
    "    os.makedirs(f\"{STORAGE_DIR}/models\")\n",
    "\n",
    "env = gym.make(args.env)\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] \n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": args.discount,\n",
    "    \"tau\": args.tau,\n",
    "    # TD3\n",
    "    \"policy_noise\": args.policy_noise * max_action,\n",
    "    \"noise_clip\": args.noise_clip * max_action,\n",
    "    \"policy_freq\": args.policy_freq,\n",
    "    # TD3 + BC\n",
    "    \"alpha\": args.alpha\n",
    "}\n",
    "\n",
    "# Initialize policy\n",
    "policy = TD3_BC.TD3_BC(**kwargs)\n",
    "\n",
    "if args.load_model != \"\":\n",
    "    policy_file = file_name if args.load_model == \"default\" else args.load_model\n",
    "    policy.load(f\"{STORAGE_DIR}/models/{policy_file}\")\n",
    "\n",
    "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f84369",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5583d",
   "metadata": {},
   "source": [
    "## Train policy on offline RL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849c9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = args.policy +\"-\"+ args.env + \"-s\" + str(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f28bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(policy='TD3_BC_100k', env='HalfCheetah-v4', seed=0, dataset_size=100000, eval_freq=5000, save_freq=5000, max_timesteps=100000, save_model=True, save_checkpoint=True, load_model='', expl_noise=0.1, batch_size=256, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2, alpha=2.5, normalize=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a84b4c8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/PycharmProjects/TD3_BC_Kabuki/venv/lib/python3.9/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mwilldudley\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "/home/will/PycharmProjects/TD3_BC_Kabuki/venv/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:58: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73b0ff73a1a4658b11fe856668047d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670786650000005, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/will/PycharmProjects/TD3_BC_Kabuki/wandb/run-20221128_095219-TD3_BC_100k-HalfCheetah-v4-s0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/willdudley/d4rl_td3/runs/TD3_BC_100k-HalfCheetah-v4-s0\" target=\"_blank\">TD3_BC_100k-HalfCheetah-v4-s0</a></strong> to <a href=\"https://wandb.ai/willdudley/d4rl_td3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/willdudley/d4rl_td3/runs/TD3_BC_100k-HalfCheetah-v4-s0?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcbd6360280>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uhhh what's this stuff\n",
    "# replay_buffer.convert_D4RL(d4rl.qlearning_dataset(env), args.dataset_size)\n",
    "#\n",
    "#\n",
    "# if args.normalize:\n",
    "#     mean,std = replay_buffer.normalize_states()\n",
    "# else:\n",
    "#     mean,std = 0,1\n",
    "\n",
    "\n",
    "import wandb as wandb_logger\n",
    "# wandb_logger.init( id = args.policy +\"-\"+ args.env + \"-s\" + str(args.seed),\n",
    "#         entity = \"willdudley\",\n",
    "#         project= \"d4rl_td3\",\n",
    "#         config = args,\n",
    "#         resume = \"allow\")\n",
    "\n",
    "wandb_logger.init( id = run_name,\n",
    "        entity = \"willdudley\",\n",
    "        project= \"d4rl_td3\",\n",
    "        config = args,\n",
    "        resume = \"allow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb2d4ff7-d54d-4de7-a435-f1f506b46015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "high <= 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [27], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m evaluations \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mint\u001B[39m(args\u001B[38;5;241m.\u001B[39mmax_timesteps))):\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreplay_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# Evaluate episode\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (t \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m args\u001B[38;5;241m.\u001B[39meval_freq \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/TD3_BC_Kabuki/TD3_BC.py:104\u001B[0m, in \u001B[0;36mTD3_BC.train\u001B[0;34m(self, replay_buffer, batch_size)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_it \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;66;03m# Sample replay buffer\u001B[39;00m\n\u001B[0;32m--> 104\u001B[0m state, action, next_state, reward, not_done \u001B[38;5;241m=\u001B[39m \u001B[43mreplay_buffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;66;03m# Select action according to policy and add clipped noise\u001B[39;00m\n\u001B[1;32m    108\u001B[0m     noise \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    109\u001B[0m             torch\u001B[38;5;241m.\u001B[39mrandn_like(action) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_noise\n\u001B[1;32m    110\u001B[0m     )\u001B[38;5;241m.\u001B[39mclamp(\u001B[38;5;241m-\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnoise_clip, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnoise_clip)\n",
      "File \u001B[0;32m~/PycharmProjects/TD3_BC_Kabuki/utils.py:35\u001B[0m, in \u001B[0;36mReplayBuffer.sample\u001B[0;34m(self, batch_size)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msample\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch_size):\n\u001B[0;32m---> 35\u001B[0m \tind \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \t\u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m     38\u001B[0m \t\ttorch\u001B[38;5;241m.\u001B[39mFloatTensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate[ind])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice),\n\u001B[1;32m     39\u001B[0m \t\ttorch\u001B[38;5;241m.\u001B[39mFloatTensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction[ind])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     42\u001B[0m \t\ttorch\u001B[38;5;241m.\u001B[39mFloatTensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnot_done[ind])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     43\u001B[0m \t)\n",
      "File \u001B[0;32mmtrand.pyx:748\u001B[0m, in \u001B[0;36mnumpy.random.mtrand.RandomState.randint\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_bounded_integers.pyx:1247\u001B[0m, in \u001B[0;36mnumpy.random._bounded_integers._rand_int64\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: high <= 0"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "for t in tqdm(range(int(args.max_timesteps))):\n",
    "    policy.train(replay_buffer, args.batch_size)\n",
    "    # Evaluate episode\n",
    "    if (t + 1) % args.eval_freq == 0:\n",
    "        print(f\"Time steps: {t+1},\")\n",
    "        evaluations.append(eval_policy(policy, args.env, args.seed, mean, std))\n",
    "        np.save(f\"{STORAGE_DIR}/results/{file_name}\", evaluations)\n",
    "\n",
    "        # wandb log\n",
    "        print(f\"logging:\",evaluations[-1])\n",
    "        wandb_logger.log({\"avg_score\":evaluations[-1], \"train_step\":t})\n",
    "\n",
    "        if args.save_model:\n",
    "            policy.save(f\"{STORAGE_DIR}/models/{file_name}\")\n",
    "\n",
    "    if (t + 1) % args.save_freq == 0:\n",
    "        print(f\"Save step: {t+1}\")\n",
    "        np.save(f\"{STORAGE_DIR}/results/{file_name}_it{t}\", evaluations)\n",
    "        if args.save_checkpoint:\n",
    "            policy.save(f\"{STORAGE_DIR}/models/{file_name}_it{t}\")\n",
    "            print(\"policy saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978fe340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110752ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
